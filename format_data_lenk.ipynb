{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd064698a2da1522c74ed3fe826d1757ccf62e586dc35e7061e53f76de1ce6ee29b",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'Traindata', 'Testdata'])\n"
     ]
    }
   ],
   "source": [
    "# Load the Lenk dataset\n",
    "lenk_name = \"data/lenk/lenk_data.mat\"\n",
    "lenk_dataset = sio.loadmat(lenk_name)\n",
    "print(lenk_dataset.keys())\n",
    "lenk_train = lenk_dataset['Traindata']\n",
    "lenk_test = lenk_dataset['Testdata']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(180, 20, 14)\n(180, 20)\n"
     ]
    }
   ],
   "source": [
    "# Convert Lenk train dataset into T x N x d1 form\n",
    "n_train = np.unique(lenk_train[:, :-1], axis=0).shape[0]\n",
    "tasks = []\n",
    "labels = []\n",
    "for task_begin in range(0, lenk_train.shape[0], n_train):\n",
    "    task_end = task_begin + n_train\n",
    "    task = lenk_train[task_begin:task_end,:-1]\n",
    "    label = lenk_train[task_begin:task_end,-1]\n",
    "    tasks.append(task)\n",
    "    labels.append(label)\n",
    "\n",
    "# Add Lenk test dataset into the mix\n",
    "n_test = np.unique(lenk_test[:, :-1], axis=0).shape[0]\n",
    "task_i = 0\n",
    "for task_begin in range(0, lenk_test.shape[0], n_test):\n",
    "    task_end = task_begin + n_test\n",
    "    task = lenk_test[task_begin:task_end,:-1]\n",
    "    label = lenk_test[task_begin:task_end,-1]\n",
    "    tasks[task_i] = np.vstack((tasks[task_i], task))\n",
    "    labels[task_i] = np.append(labels[task_i], label)\n",
    "    task_i += 1\n",
    "tasks = np.asarray(tasks)\n",
    "print(tasks.shape)\n",
    "labels = np.asarray(labels)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "T = 180\nd1 = 20\n"
     ]
    }
   ],
   "source": [
    "# Extract the initial parameters\n",
    "T = tasks.shape[0]\n",
    "d1 = tasks[0].shape[0]\n",
    "print(\"T = {}\".format(T))\n",
    "print(\"d1 = {}\".format(d1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_Y(task, labels):\n",
    "    d2 = 2 * task.shape[1]\n",
    "    sigma = 100\n",
    "\n",
    "    # Generate the parameters for the transform\n",
    "    sum_phi = np.zeros(d2)\n",
    "    for i in range(task.shape[0]):\n",
    "        vec_operand = np.reshape(task[i], (14, 1)) @ np.reshape(np.transpose(np.array([labels[i], 1])), (1, 2))\n",
    "        vec_op_1 = vec_operand[:,0]\n",
    "        vec_op_2 = vec_operand[:,1]\n",
    "        phi = np.concatenate((vec_op_1, vec_op_2))\n",
    "        sum_phi += phi\n",
    "    avg_phi = np.divide(sum_phi, task.shape[0])\n",
    "    return avg_phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[Errno 17] File exists: 'data/lenk/N2_2/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_4/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_6/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_8/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_10/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_12/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_14/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_16/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_18/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n",
      "[Errno 17] File exists: 'data/lenk/N2_20/'\n",
      "(3580, 1)\n",
      "(3580, 14)\n",
      "(179, 28)\n"
     ]
    }
   ],
   "source": [
    "# Choose test task and the rest is training\n",
    "test_task = tasks[0]        # Can also choose this randomly\n",
    "test_labels = labels[0]\n",
    "train_tasks = tasks[1:]\n",
    "train_labels = labels[1:]\n",
    "\n",
    "# Loop over N2 for this new test task\n",
    "parent_dir = \"data/lenk/\"\n",
    "for N2 in range(2, 22, 2):\n",
    "    # Get and store the test data\n",
    "    random_indices = np.random.choice(test_task.shape[0], size=N2, replace=False)\n",
    "    X0 = test_task[random_indices, :]\n",
    "    Y0 = transform_Y(test_task, test_labels) # Independent of X\n",
    "    R0 = test_labels[random_indices]\n",
    "    path = os.path.join(parent_dir, \"N2_{}/\".format(N2))\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    pickle.dump(X0, open(path + \"X.pkl\", \"wb\"))\n",
    "    pickle.dump(Y0, open(path + \"Y0.pkl\", \"wb\"))\n",
    "    pickle.dump(R0, open(path + \"R0.pkl\", \"wb\"))\n",
    "\n",
    "    # Get and store the training data\n",
    "    d1 = test_task.shape[1]\n",
    "    d2 = 2 * d1\n",
    "    X_full = np.ones((1, d1))\n",
    "    Y_full = np.ones((1, d2))\n",
    "    R_full = np.ones((1, 1))\n",
    "    task_function = []\n",
    "    index_total = 0\n",
    "    for i in range(train_tasks.shape[0]):\n",
    "        Y = transform_Y(train_tasks[i], train_labels[i])\n",
    "        Y_full = np.vstack((Y_full, Y))\n",
    "        for j in range(test_task.shape[0]):\n",
    "            X = train_tasks[i][j]\n",
    "            R = train_labels[i][j]\n",
    "            X_full = np.vstack((X_full, X))\n",
    "            R_full = np.vstack((R_full, R))\n",
    "\n",
    "            prev_index_total = index_total\n",
    "            index_total += X.shape[0]\n",
    "            for j in range(prev_index_total, index_total):\n",
    "                task_function.append(i)\n",
    "\n",
    "    task_function = np.asarray(task_function)\n",
    "    R_full = R_full[1:]     # N x 1\n",
    "    print(R_full.shape)\n",
    "    X_full = X_full[1:]     # N x d1\n",
    "    print(X_full.shape)\n",
    "    Y_full = Y_full[1:]     # T x d2\n",
    "    print(Y_full.shape)\n",
    "    pickle.dump(X_full, open(path + \"X.pkl\", \"wb\"))\n",
    "    pickle.dump(Y_full, open(path + \"Y.pkl\", \"wb\"))\n",
    "    pickle.dump(R_full, open(path + \"R.pkl\", \"wb\"))\n",
    "    pickle.dump(task_function, open(path + \"task_function.pkl\", \"wb\"))"
   ]
  }
 ]
}