{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd064698a2da1522c74ed3fe826d1757ccf62e586dc35e7061e53f76de1ce6ee29b",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "source": [
    "# Load the Schools dataset\n",
    "schools_name = \"data/school/schoolData.mat\"\n",
    "schools_dataset = sio.loadmat(schools_name)\n",
    "print(schools_dataset.keys())\n",
    "X_schools = schools_dataset['X']\n",
    "X_schools = np.squeeze(X_schools)\n",
    "Y_schools = schools_dataset['Y']\n",
    "Y_schools = np.squeeze(Y_schools)\n",
    "print(X_schools.shape)\n",
    "print(X_schools[0].shape)\n",
    "print(Y_schools.shape)\n",
    "print(Y_schools[0].shape)\n",
    "\n",
    "T = X_schools.shape[0]\n",
    "d1 = X_schools[0].shape[0]\n",
    "print(\"T = {}\".format(T))\n",
    "print(\"d1 = {}\".format(d1))"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'X', 'Y'])\n(139,)\n(26, 200)\n(139,)\n(200, 1)\nT = 139\nd1 = 26\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract 10 meta-test sets of varying sizes (from 20 to 200)\n",
    "samples_per_task = {}\n",
    "for i in range(T):\n",
    "    for k in range(20, 220, 20):\n",
    "        if (k <= X_schools[i].shape[1] < k + 10 and k not in samples_per_task):\n",
    "            samples_per_task[k] = i\n",
    "\n",
    "# Key: number of samples for test set\n",
    "# Value: the i'th task that contains that number of samples\n",
    "samples_per_task = dict(sorted(samples_per_task.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(task, X_schools, Y_schools):\n",
    "    test_task = X_schools[task]\n",
    "    train_tasks = np.delete(X_schools, task)\n",
    "    test_labels = Y_schools[task]\n",
    "    train_labels = np.delete(Y_schools, task)\n",
    "    return train_tasks, test_task, train_labels, test_labels\n",
    "\n",
    "def transform_Y(X):\n",
    "    d2 = 1000\n",
    "    sigma = 100\n",
    "\n",
    "    # Generate the parameters for the transform\n",
    "    sum_phi = np.zeros(d2)\n",
    "    v = np.random.uniform(0, 2 * np.pi, size=d2)\n",
    "    U = np.random.normal(0, sigma, size=(d2, X.shape[1]))\n",
    "    for i in range(X.shape[0]):\n",
    "        sum_phi += np.sqrt(2/d2) * np.cos(U @ X[i] + v)\n",
    "    avg_phi = np.divide(sum_phi, X.shape[0])\n",
    "    return avg_phi\n",
    "\n",
    "def generate_X_Y(task):\n",
    "    X = np.transpose(task)\n",
    "    Y = transform_Y(X)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a training set by separating out the meta-test set and keeping the rest\n",
    "parent_dir = \"data/school/\"\n",
    "N = 2000\n",
    "for n_t in samples_per_task:\n",
    "    train_tasks, test_task, train_labels, test_labels = train_test_split(samples_per_task[n_t], X_schools, Y_schools)\n",
    "\n",
    "    # Get the meta-test task and save it\n",
    "    #TODO: Y should be T x d2, NOT N x d2\n",
    "    X0, Y0 = generate_X_Y(test_task)\n",
    "    R0 = test_labels\n",
    "    path = os.path.join(parent_dir, \"N2_{}/\".format(n_t))\n",
    "    try:\n",
    "        os.mkdir(path)\n",
    "    except OSError as error:\n",
    "        print(error)\n",
    "    pickle.dump(X0, open(path + \"X0\", \"wb\"))\n",
    "    pickle.dump(Y0, open(path + \"Y0\", \"wb\"))\n",
    "    pickle.dump(R0, open(path + \"R0\", \"wb\"))\n",
    "\n",
    "    # Get the rest of the training data and save all of it\n",
    "    d1 = test_task.shape[0]\n",
    "    d2 = 1000\n",
    "    X_full = np.ones((1, d1))\n",
    "    Y_full = np.ones((1, d2))\n",
    "    R_full = np.ones((1, 1))\n",
    "\n",
    "    # For each remaining task (138, in this case)\n",
    "    task_function = []\n",
    "    index_total = 0\n",
    "    for i in range(train_tasks.shape[0]):\n",
    "        train_task = train_tasks[i]\n",
    "        train_label = train_labels[i]\n",
    "        X, Y = generate_X_Y(train_task)\n",
    "        R = train_label\n",
    "        X_full = np.vstack((X_full, X))\n",
    "        Y_full = np.vstack((Y_full, Y))\n",
    "        R_full = np.vstack((R_full, R))\n",
    "\n",
    "        prev_index_total = index_total\n",
    "        index_total += X.shape[0]\n",
    "        for j in range(prev_index_total, index_total):\n",
    "            task_function.append(i)\n",
    "\n",
    "        # Record the task mapping for each sample in this task\n",
    "        if (X_full.shape[0] > N):\n",
    "            break\n",
    "\n",
    "    task_function = np.asarray(task_function)\n",
    "    R_full = R_full[1:]     # N x 1\n",
    "    X_full = X_full[1:]     # N x d1\n",
    "    Y_full = Y_full[1:]     # T x d2\n",
    "    pickle.dump(X_full, open(path + \"X.pkl\", \"wb\"))\n",
    "    pickle.dump(Y_full, open(path + \"Y.pkl\", \"wb\"))\n",
    "    pickle.dump(R_full, open(path + \"R.pkl\", \"wb\"))\n",
    "    pickle.dump(task_function, open(path + \"task_function.pkl\", \"wb\"))\n",
    "\n",
    "    break"
   ]
  }
 ]
}